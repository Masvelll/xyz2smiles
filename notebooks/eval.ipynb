{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c95a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user12/.conda/envs/pasha/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a4367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = json.loads(open('/home/user12/prompts/valid.jsonl').readline())          # одна строка\n",
    "messages = sample[\"messages\"]                                # list[dict]\n",
    "\n",
    "# Обрезаем всё после последнего user\n",
    "messages_no_answer = [\n",
    "    m for m in messages if m[\"role\"] != \"assistant\"\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages_no_answer,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True        # «<|im_start|>assistant\\n» в конце\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df3cdb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a chemist. For each atom pair within 2 Å classify the bond type. Labels: 0 no-bond, 1 single, 2 double, 3 triple, 4 aromatic. Return ONLY JSON list [{\"pair\":[i,j],\"label\":n}].'},\n",
       " {'role': 'user',\n",
       "  'content': 'Pairs within 2 Å:\\n[0,0]=<p077>\\n[0,1]=<p121>\\n[1,0]=<p046>\\n[1,1]=<p117>\\n[1,2]=<p115>\\n[2,1]=<p115>\\n[2,2]=<p060>\\n[2,3]=<p113>\\n[3,2]=<p115>\\n[3,3]=<p025>\\n[3,4]=<p124>\\n[3,5]=<p083>\\n[4,3]=<p010>\\n[4,4]=<p117>\\n[5,3]=<p083>\\n[5,5]=<p025>\\n[5,6]=<p107>\\n[5,27]=<p065>\\n[6,5]=<p028>\\n[6,6]=<p117>\\n[6,7]=<p115>\\n[7,6]=<p115>\\n[7,7]=<p060>\\n[7,8]=<p115>\\n[8,7]=<p115>\\n[8,8]=<p117>\\n[8,9]=<p016>\\n[8,26]=<p086>\\n[9,8]=<p038>\\n[9,9]=<p034>\\n[9,10]=<p017>\\n[9,11]=<p096>\\n[10,9]=<p016>\\n[10,10]=<p117>\\n[11,9]=<p032>\\n[11,11]=<p025>\\n[11,12]=<p120>\\n[12,11]=<p120>\\n[12,12]=<p060>\\n[12,13]=<p016>\\n[12,25]=<p104>\\n[13,12]=<p038>\\n[13,13]=<p034>\\n[13,14]=<p075>\\n[13,15]=<p096>\\n[13,16]=<p019>\\n[14,13]=<p096>\\n[14,14]=<p025>\\n[15,13]=<p099>\\n[15,15]=<p025>\\n[16,13]=<p099>\\n[16,16]=<p025>\\n[16,17]=<p058>\\n[16,21]=<p127>\\n[17,16]=<p001>\\n[17,17]=<p060>\\n[17,18]=<p037>\\n[18,17]=<p037>\\n[18,18]=<p060>\\n[18,19]=<p029>\\n[18,24]=<p040>\\n[19,18]=<p002>\\n[19,19]=<p060>\\n[19,20]=<p029>\\n[20,19]=<p029>\\n[20,20]=<p060>\\n[20,21]=<p029>\\n[21,16]=<p114>\\n[21,20]=<p029>\\n[21,21]=<p060>\\n[21,22]=<p002>\\n[22,21]=<p002>\\n[22,22]=<p060>\\n[22,23]=<p115>\\n[23,22]=<p048>\\n[23,23]=<p025>\\n[24,18]=<p040>\\n[24,24]=<p117>\\n[25,12]=<p110>\\n[25,25]=<p060>\\n[26,8]=<p086>\\n[26,26]=<p060>\\n[26,27]=<p076>\\n[27,5]=<p081>\\n[27,26]=<p039>\\n[27,27]=<p117>\\n\\nAtoms (index→type): 0:C, 1:C, 2:O, 3:C, 4:O, 5:C, 6:C, 7:C, 8:N, 9:C, 10:O, 11:C, 12:N, 13:S, 14:O, 15:O, 16:C, 17:C, 18:C, 19:C, 20:C, 21:C, 22:O, 23:C, 24:C, 25:C, 26:C, 27:C'},\n",
       " {'role': 'assistant', 'content': ''}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_no_answer.append({'role': 'assistant', 'content': ''})\n",
    "messages_no_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2993839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base = \"Qwen/Qwen3-0.6B\"\n",
    "ckpt = \"runs/lora_bond_qwen/checkpoint-300\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(base, device_map=\"auto\", trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, ckpt)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dea90c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a chemist. For each atom pair within 2 Å classify the bond type. Labels: 0 no-bond, 1 single, 2 double, 3 triple, 4 aromatic. Return ONLY JSON list [{\"pair\":[i,j],\"label\":n}].<|im_end|>\\n<|im_start|>user\\nPairs within 2 Å:\\n[0,0]=<p077>\\n[0,1]=<p121>\\n[1,0]=<p046>\\n[1,1]=<p117>\\n[1,2]=<p115>\\n[2,1]=<p115>\\n[2,2]=<p060>\\n[2,3]=<p113>\\n[3,2]=<p115>\\n[3,3]=<p025>\\n[3,4]=<p124>\\n[3,5]=<p083>\\n[4,3]=<p010>\\n[4,4]=<p117>\\n[5,3]=<p083>\\n[5,5]=<p025>\\n[5,6]=<p107>\\n[5,27]=<p065>\\n[6,5]=<p028>\\n[6,6]=<p117>\\n[6,7]=<p115>\\n[7,6]=<p115>\\n[7,7]=<p060>\\n[7,8]=<p115>\\n[8,7]=<p115>\\n[8,8]=<p117>\\n[8,9]=<p016>\\n[8,26]=<p086>\\n[9,8]=<p038>\\n[9,9]=<p034>\\n[9,10]=<p017>\\n[9,11]=<p096>\\n[10,9]=<p016>\\n[10,10]=<p117>\\n[11,9]=<p032>\\n[11,11]=<p025>\\n[11,12]=<p120>\\n[12,11]=<p120>\\n[12,12]=<p060>\\n[12,13]=<p016>\\n[12,25]=<p104>\\n[13,12]=<p038>\\n[13,13]=<p034>\\n[13,14]=<p075>\\n[13,15]=<p096>\\n[13,16]=<p019>\\n[14,13]=<p096>\\n[14,14]=<p025>\\n[15,13]=<p099>\\n[15,15]=<p025>\\n[16,13]=<p099>\\n[16,16]=<p025>\\n[16,17]=<p058>\\n[16,21]=<p127>\\n[17,16]=<p001>\\n[17,17]=<p060>\\n[17,18]=<p037>\\n[18,17]=<p037>\\n[18,18]=<p060>\\n[18,19]=<p029>\\n[18,24]=<p040>\\n[19,18]=<p002>\\n[19,19]=<p060>\\n[19,20]=<p029>\\n[20,19]=<p029>\\n[20,20]=<p060>\\n[20,21]=<p029>\\n[21,16]=<p114>\\n[21,20]=<p029>\\n[21,21]=<p060>\\n[21,22]=<p002>\\n[22,21]=<p002>\\n[22,22]=<p060>\\n[22,23]=<p115>\\n[23,22]=<p048>\\n[23,23]=<p025>\\n[24,18]=<p040>\\n[24,24]=<p117>\\n[25,12]=<p110>\\n[25,25]=<p060>\\n[26,8]=<p086>\\n[26,26]=<p060>\\n[26,27]=<p076>\\n[27,5]=<p081>\\n[27,26]=<p039>\\n[27,27]=<p117>\\n\\nAtoms (index→type): 0:C, 1:C, 2:O, 3:C, 4:O, 5:C, 6:C, 7:C, 8:N, 9:C, 10:O, 11:C, 12:N, 13:S, 14:O, 15:O, 16:C, 17:C, 18:C, 19:C, 20:C, 21:C, 22:O, 23:C, 24:C, 25:C, 26:C, 27:C<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04ce207f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,  ...,  55992, 151645,    198]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9aca273",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages_no_answer,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False  # важно оставить False для совместимости\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9203bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages_no_answer,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=1024,  # ограничим генерацию разумным числом\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(\n",
    "    output[0][input_ids.shape[-1]:],\n",
    "    skip_special_tokens=True\n",
    ").strip()\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e79e3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerated_text\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_text' is not defined"
     ]
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user12/.conda/envs/pasha/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.tok_embeddings\n",
      "model.layers.0.attention.wqkv\n",
      "model.layers.0.attention.wo\n",
      "model.layers.0.feed_forward.w1\n",
      "model.layers.0.feed_forward.w3\n",
      "model.layers.0.feed_forward.w2\n",
      "model.layers.0.attention_norm\n",
      "model.layers.0.ffn_norm\n",
      "model.layers.1.attention.wqkv\n",
      "model.layers.1.attention.wo\n",
      "model.layers.1.feed_forward.w1\n",
      "model.layers.1.feed_forward.w3\n",
      "model.layers.1.feed_forward.w2\n",
      "model.layers.1.attention_norm\n",
      "model.layers.1.ffn_norm\n",
      "model.layers.2.attention.wqkv\n",
      "model.layers.2.attention.wo\n",
      "model.layers.2.feed_forward.w1\n",
      "model.layers.2.feed_forward.w3\n",
      "model.layers.2.feed_forward.w2\n",
      "model.layers.2.attention_norm\n",
      "model.layers.2.ffn_norm\n",
      "model.layers.3.attention.wqkv\n",
      "model.layers.3.attention.wo\n",
      "model.layers.3.feed_forward.w1\n",
      "model.layers.3.feed_forward.w3\n",
      "model.layers.3.feed_forward.w2\n",
      "model.layers.3.attention_norm\n",
      "model.layers.3.ffn_norm\n",
      "model.layers.4.attention.wqkv\n",
      "model.layers.4.attention.wo\n",
      "model.layers.4.feed_forward.w1\n",
      "model.layers.4.feed_forward.w3\n",
      "model.layers.4.feed_forward.w2\n",
      "model.layers.4.attention_norm\n",
      "model.layers.4.ffn_norm\n",
      "model.layers.5.attention.wqkv\n",
      "model.layers.5.attention.wo\n",
      "model.layers.5.feed_forward.w1\n",
      "model.layers.5.feed_forward.w3\n",
      "model.layers.5.feed_forward.w2\n",
      "model.layers.5.attention_norm\n",
      "model.layers.5.ffn_norm\n",
      "model.layers.6.attention.wqkv\n",
      "model.layers.6.attention.wo\n",
      "model.layers.6.feed_forward.w1\n",
      "model.layers.6.feed_forward.w3\n",
      "model.layers.6.feed_forward.w2\n",
      "model.layers.6.attention_norm\n",
      "model.layers.6.ffn_norm\n",
      "model.layers.7.attention.wqkv\n",
      "model.layers.7.attention.wo\n",
      "model.layers.7.feed_forward.w1\n",
      "model.layers.7.feed_forward.w3\n",
      "model.layers.7.feed_forward.w2\n",
      "model.layers.7.attention_norm\n",
      "model.layers.7.ffn_norm\n",
      "model.layers.8.attention.wqkv\n",
      "model.layers.8.attention.wo\n",
      "model.layers.8.feed_forward.w1\n",
      "model.layers.8.feed_forward.w3\n",
      "model.layers.8.feed_forward.w2\n",
      "model.layers.8.attention_norm\n",
      "model.layers.8.ffn_norm\n",
      "model.layers.9.attention.wqkv\n",
      "model.layers.9.attention.wo\n",
      "model.layers.9.feed_forward.w1\n",
      "model.layers.9.feed_forward.w3\n",
      "model.layers.9.feed_forward.w2\n",
      "model.layers.9.attention_norm\n",
      "model.layers.9.ffn_norm\n",
      "model.layers.10.attention.wqkv\n",
      "model.layers.10.attention.wo\n",
      "model.layers.10.feed_forward.w1\n",
      "model.layers.10.feed_forward.w3\n",
      "model.layers.10.feed_forward.w2\n",
      "model.layers.10.attention_norm\n",
      "model.layers.10.ffn_norm\n",
      "model.layers.11.attention.wqkv\n",
      "model.layers.11.attention.wo\n",
      "model.layers.11.feed_forward.w1\n",
      "model.layers.11.feed_forward.w3\n",
      "model.layers.11.feed_forward.w2\n",
      "model.layers.11.attention_norm\n",
      "model.layers.11.ffn_norm\n",
      "model.layers.12.attention.wqkv\n",
      "model.layers.12.attention.wo\n",
      "model.layers.12.feed_forward.w1\n",
      "model.layers.12.feed_forward.w3\n",
      "model.layers.12.feed_forward.w2\n",
      "model.layers.12.attention_norm\n",
      "model.layers.12.ffn_norm\n",
      "model.layers.13.attention.wqkv\n",
      "model.layers.13.attention.wo\n",
      "model.layers.13.feed_forward.w1\n",
      "model.layers.13.feed_forward.w3\n",
      "model.layers.13.feed_forward.w2\n",
      "model.layers.13.attention_norm\n",
      "model.layers.13.ffn_norm\n",
      "model.layers.14.attention.wqkv\n",
      "model.layers.14.attention.wo\n",
      "model.layers.14.feed_forward.w1\n",
      "model.layers.14.feed_forward.w3\n",
      "model.layers.14.feed_forward.w2\n",
      "model.layers.14.attention_norm\n",
      "model.layers.14.ffn_norm\n",
      "model.layers.15.attention.wqkv\n",
      "model.layers.15.attention.wo\n",
      "model.layers.15.feed_forward.w1\n",
      "model.layers.15.feed_forward.w3\n",
      "model.layers.15.feed_forward.w2\n",
      "model.layers.15.attention_norm\n",
      "model.layers.15.ffn_norm\n",
      "model.layers.16.attention.wqkv\n",
      "model.layers.16.attention.wo\n",
      "model.layers.16.feed_forward.w1\n",
      "model.layers.16.feed_forward.w3\n",
      "model.layers.16.feed_forward.w2\n",
      "model.layers.16.attention_norm\n",
      "model.layers.16.ffn_norm\n",
      "model.layers.17.attention.wqkv\n",
      "model.layers.17.attention.wo\n",
      "model.layers.17.feed_forward.w1\n",
      "model.layers.17.feed_forward.w3\n",
      "model.layers.17.feed_forward.w2\n",
      "model.layers.17.attention_norm\n",
      "model.layers.17.ffn_norm\n",
      "model.layers.18.attention.wqkv\n",
      "model.layers.18.attention.wo\n",
      "model.layers.18.feed_forward.w1\n",
      "model.layers.18.feed_forward.w3\n",
      "model.layers.18.feed_forward.w2\n",
      "model.layers.18.attention_norm\n",
      "model.layers.18.ffn_norm\n",
      "model.layers.19.attention.wqkv\n",
      "model.layers.19.attention.wo\n",
      "model.layers.19.feed_forward.w1\n",
      "model.layers.19.feed_forward.w3\n",
      "model.layers.19.feed_forward.w2\n",
      "model.layers.19.attention_norm\n",
      "model.layers.19.ffn_norm\n",
      "model.layers.20.attention.wqkv\n",
      "model.layers.20.attention.wo\n",
      "model.layers.20.feed_forward.w1\n",
      "model.layers.20.feed_forward.w3\n",
      "model.layers.20.feed_forward.w2\n",
      "model.layers.20.attention_norm\n",
      "model.layers.20.ffn_norm\n",
      "model.layers.21.attention.wqkv\n",
      "model.layers.21.attention.wo\n",
      "model.layers.21.feed_forward.w1\n",
      "model.layers.21.feed_forward.w3\n",
      "model.layers.21.feed_forward.w2\n",
      "model.layers.21.attention_norm\n",
      "model.layers.21.ffn_norm\n",
      "model.layers.22.attention.wqkv\n",
      "model.layers.22.attention.wo\n",
      "model.layers.22.feed_forward.w1\n",
      "model.layers.22.feed_forward.w3\n",
      "model.layers.22.feed_forward.w2\n",
      "model.layers.22.attention_norm\n",
      "model.layers.22.ffn_norm\n",
      "model.layers.23.attention.wqkv\n",
      "model.layers.23.attention.wo\n",
      "model.layers.23.feed_forward.w1\n",
      "model.layers.23.feed_forward.w3\n",
      "model.layers.23.feed_forward.w2\n",
      "model.layers.23.attention_norm\n",
      "model.layers.23.ffn_norm\n",
      "model.layers.24.attention.wqkv\n",
      "model.layers.24.attention.wo\n",
      "model.layers.24.feed_forward.w1\n",
      "model.layers.24.feed_forward.w3\n",
      "model.layers.24.feed_forward.w2\n",
      "model.layers.24.attention_norm\n",
      "model.layers.24.ffn_norm\n",
      "model.layers.25.attention.wqkv\n",
      "model.layers.25.attention.wo\n",
      "model.layers.25.feed_forward.w1\n",
      "model.layers.25.feed_forward.w3\n",
      "model.layers.25.feed_forward.w2\n",
      "model.layers.25.attention_norm\n",
      "model.layers.25.ffn_norm\n",
      "model.layers.26.attention.wqkv\n",
      "model.layers.26.attention.wo\n",
      "model.layers.26.feed_forward.w1\n",
      "model.layers.26.feed_forward.w3\n",
      "model.layers.26.feed_forward.w2\n",
      "model.layers.26.attention_norm\n",
      "model.layers.26.ffn_norm\n",
      "model.layers.27.attention.wqkv\n",
      "model.layers.27.attention.wo\n",
      "model.layers.27.feed_forward.w1\n",
      "model.layers.27.feed_forward.w3\n",
      "model.layers.27.feed_forward.w2\n",
      "model.layers.27.attention_norm\n",
      "model.layers.27.ffn_norm\n",
      "model.layers.28.attention.wqkv\n",
      "model.layers.28.attention.wo\n",
      "model.layers.28.feed_forward.w1\n",
      "model.layers.28.feed_forward.w3\n",
      "model.layers.28.feed_forward.w2\n",
      "model.layers.28.attention_norm\n",
      "model.layers.28.ffn_norm\n",
      "model.layers.29.attention.wqkv\n",
      "model.layers.29.attention.wo\n",
      "model.layers.29.feed_forward.w1\n",
      "model.layers.29.feed_forward.w3\n",
      "model.layers.29.feed_forward.w2\n",
      "model.layers.29.attention_norm\n",
      "model.layers.29.ffn_norm\n",
      "model.layers.30.attention.wqkv\n",
      "model.layers.30.attention.wo\n",
      "model.layers.30.feed_forward.w1\n",
      "model.layers.30.feed_forward.w3\n",
      "model.layers.30.feed_forward.w2\n",
      "model.layers.30.attention_norm\n",
      "model.layers.30.ffn_norm\n",
      "model.layers.31.attention.wqkv\n",
      "model.layers.31.attention.wo\n",
      "model.layers.31.feed_forward.w1\n",
      "model.layers.31.feed_forward.w3\n",
      "model.layers.31.feed_forward.w2\n",
      "model.layers.31.attention_norm\n",
      "model.layers.31.ffn_norm\n",
      "model.norm\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"AI4Chem/ChemLLM-7B-Chat-1_5-DPO\", trust_remote_code=True)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, \"weight\") or hasattr(module, \"in_features\"):\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5cba905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a chemist. For each atom pair within 2 Å classify the bond type. Labels: 0 no-bond, 1 single, 2 double, 3 triple, 4 aromatic. Return ONLY JSON list [{\"pair\":[i,j],\"label\":n}].<|im_end|>\n",
      "<|im_start|>user\n",
      "Pairs within 2 Å:\n",
      "[0,0]=<p077>\n",
      "[0,1]=<p121>\n",
      "[1,0]=<p046>\n",
      "[1,1]=<p117>\n",
      "[1,2]=<p115>\n",
      "[2,1]=<p115>\n",
      "[2,2]=<p060>\n",
      "[2,3]=<p113>\n",
      "[3,2]=<p115>\n",
      "[3,3]=<p025>\n",
      "[3,4]=<p124>\n",
      "[3,5]=<p083>\n",
      "[4,3]=<p010>\n",
      "[4,4]=<p117>\n",
      "[5,3]=<p083>\n",
      "[5,5]=<p025>\n",
      "[5,6]=<p107>\n",
      "[5,27]=<p065>\n",
      "[6,5]=<p028>\n",
      "[6,6]=<p117>\n",
      "[6,7]=<p115>\n",
      "[7,6]=<p115>\n",
      "[7,7]=<p060>\n",
      "[7,8]=<p115>\n",
      "[8,7]=<p115>\n",
      "[8,8]=<p117>\n",
      "[8,9]=<p016>\n",
      "[8,26]=<p086>\n",
      "[9,8]=<p038>\n",
      "[9,9]=<p034>\n",
      "[9,10]=<p017>\n",
      "[9,11]=<p096>\n",
      "[10,9]=<p016>\n",
      "[10,10]=<p117>\n",
      "[11,9]=<p032>\n",
      "[11,11]=<p025>\n",
      "[11,12]=<p120>\n",
      "[12,11]=<p120>\n",
      "[12,12]=<p060>\n",
      "[12,13]=<p016>\n",
      "[12,25]=<p104>\n",
      "[13,12]=<p038>\n",
      "[13,13]=<p034>\n",
      "[13,14]=<p075>\n",
      "[13,15]=<p096>\n",
      "[13,16]=<p019>\n",
      "[14,13]=<p096>\n",
      "[14,14]=<p025>\n",
      "[15,13]=<p099>\n",
      "[15,15]=<p025>\n",
      "[16,13]=<p099>\n",
      "[16,16]=<p025>\n",
      "[16,17]=<p058>\n",
      "[16,21]=<p127>\n",
      "[17,16]=<p001>\n",
      "[17,17]=<p060>\n",
      "[17,18]=<p037>\n",
      "[18,17]=<p037>\n",
      "[18,18]=<p060>\n",
      "[18,19]=<p029>\n",
      "[18,24]=<p040>\n",
      "[19,18]=<p002>\n",
      "[19,19]=<p060>\n",
      "[19,20]=<p029>\n",
      "[20,19]=<p029>\n",
      "[20,20]=<p060>\n",
      "[20,21]=<p029>\n",
      "[21,16]=<p114>\n",
      "[21,20]=<p029>\n",
      "[21,21]=<p060>\n",
      "[21,22]=<p002>\n",
      "[22,21]=<p002>\n",
      "[22,22]=<p060>\n",
      "[22,23]=<p115>\n",
      "[23,22]=<p048>\n",
      "[23,23]=<p025>\n",
      "[24,18]=<p040>\n",
      "[24,24]=<p117>\n",
      "[25,12]=<p110>\n",
      "[25,25]=<p060>\n",
      "[26,8]=<p086>\n",
      "[26,26]=<p060>\n",
      "[26,27]=<p076>\n",
      "[27,5]=<p081>\n",
      "[27,26]=<p039>\n",
      "[27,27]=<p117>\n",
      "\n",
      "Atoms (index→type): 0:C, 1:C, 2:O, 3:C, 4:O, 5:C, 6:C, 7:C, 8:N, 9:C, 10:O, 11:C, 12:N, 13:S, 14:O, 15:O, 16:C, 17:C, 18:C, 19:C, 20:C, 21:C, 22:O, 23:C, 24:C, 25:C, 26:C, 27:C<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.convert_tokens_to_ids(\"<|im_end|>\")) \n",
    "# print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(tokenizer(prompt)[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eaaefb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'im' in generated\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pasha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
