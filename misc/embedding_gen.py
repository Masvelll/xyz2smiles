import sys 
import torch
import numpy as np
import json

from tqdm import tqdm

from sklearn.cluster import MiniBatchKMeans
from transformers import AutoTokenizer, AutoModelForCausalLM


pairs = torch.load('../burov/unimol_embeddings_project/geom_train_pair_embeddings.pt', weights_only=True)
geom = torch.load('./datasets/geom_train.pt', weights_only=True)


k         = 128
batchsize = 10_000           # —Å–∫–æ–ª—å–∫–æ 64-–≤–µ–∫—Ç–æ—Ä–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞ —Ä–∞–∑
max_pairs = 10_000_000          # –≤—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö ¬´–¥–æ–≤–µ–¥—ë–º¬ª —Ü–µ–Ω—Ç—Ä—ã

kmeans = MiniBatchKMeans(n_clusters=k,
                         batch_size=batchsize,
                         init_size=k*3,        # –º–æ–∂–Ω–æ –ø–æ–±–æ–ª—å—à–µ –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
                         verbose=0,
                         random_state=42)

seen = 0
for mol in tqdm(pairs):                   # pair_list: list[T(N,N,64)]
    vecs = mol.reshape(-1, 64)                # (N¬≤,64)   ‚Äì –≤ gpu/cpu –ø–∞–º—è—Ç–∏ –º–æ–ª–µ–∫—É–ª—ã
    # --- —Å–ª—É—á–∞–π–Ω–æ –±–µ—Ä—ë–º –Ω–µ –±–æ–ª—å—à–µ batchsize –≤–µ–∫—Ç–æ—Ä–æ–≤ ---
    if vecs.size(0) > batchsize:
        idx = torch.randperm(vecs.size(0))[:batchsize]
        vecs = vecs[idx]

    kmeans.partial_fit(vecs.cpu().numpy())    # —É—á–∏–º –Ω–∞ CPU, –ø–æ –∫—É—Å–æ—á–∫–∞–º
    seen += vecs.size(0)
    if seen >= max_pairs:                     # —Ö–≤–∞—Ç–∏—Ç –ø—Ä–∏–º–µ—Ä–æ–≤ ‚Äì –≤—ã—Ö–æ–¥–∏–º
        break

centers = torch.tensor(kmeans.cluster_centers_)   # (128,64)  ‚Ä¢ –≥–æ—Ç–æ–≤–æ
torch.save(centers, "centers_pair128.pt")


def vec2tok(v: torch.Tensor) -> str | list[str]:
    """
    v : Tensor (64,)       ‚Üí '<p042>'
        Tensor (M,64)      ‚Üí ['<p042>', '<p118>', ...] –¥–ª–∏–Ω–æ–π M
    """
    v = v.to(centers)                       # —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –Ω–∞ —Ç–æ–º –∂–µ –¥–µ–≤–∞–π—Å–µ

    if v.ndim == 1:                         # –æ–¥–∏–Ω–æ—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        idx = torch.cdist(v[None], centers).argmin().item()
        return f"<p{idx:03d}>"

    # –±–∞—Ç—á M√ó64  ‚Üí M –∏–Ω–¥–µ–∫—Å–æ–≤
    idx = torch.cdist(v, centers).argmin(dim=1).tolist()   # List[int] –¥–ª–∏–Ω–æ–π M
    return [f"<p{i:03d}>" for i in idx]                    # List[str]


model_name = "Qwen/Qwen1.5-1.8B-Chat"   # –∏–ª–∏ –¥—Ä—É–≥–∞—è
tokenizer  = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model      = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)

# 128 –Ω–æ–≤—ã—Ö ¬´pair-—Ç–æ–∫–µ–Ω–æ–≤¬ª
new_tokens = [f"<p{i:03d}>" for i in range(128)]

num_added = tokenizer.add_tokens(new_tokens, special_tokens=False)
print("–î–æ–±–∞–≤–∏–ª–∏:", num_added)      # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 128
model.resize_token_embeddings(len(tokenizer))   # üîë —Ä–∞—Å—à–∏—Ä—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥



SYSTEM = ("You are a chemist. For each atom pair within 2 √Ö classify the "
          "bond type. Labels: 0 no-bond, 1 single, 2 double, 3 triple, 4 aromatic. "
          "Return ONLY JSON list [{\"pair\":[i,j],\"label\":n}].")

ORDER2LABEL = {0:1, 1:2, 2:3, 3:4, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1}
ALLOWED = ['C', 'O', 'N', 'F', 'S', 'Cl', 'Br', 'I', 'P']

dev = 'cuda'
centers_gpu = centers.to(dev)
with open("bond_prompts.jsonl", "w") as fout:
    
    for mol, pair in tqdm(zip(geom, pairs), total=len(geom)):
        xyz   = mol["positions"].to(dev, non_blocking=True)      # (N,3)
        types = mol["one_hot"].argmax(-1)                        # –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∞ CPU
        pair  = pair.to(dev, non_blocking=True)                  # (N,N,64)

        dmat  = torch.cdist(xyz, xyz)                            # GPU, fp32
        i_idx, j_idx = (dmat<=2.0).nonzero(as_tuple=True)        # —Ç–æ–∂–µ GPU
        if not len(i_idx):  continue

        vecs   = pair[i_idx, j_idx]                              # (M,64)_gpu
        idx    = torch.cdist(vecs, centers_gpu).argmin(dim=1)        # (M,)_gpu
        toks   = [f"<p{i:03d}>" for i in idx.tolist()]                            # List[str]  (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π!)

        lines = [f"[{i},{j}]={tok}"
                for (i,j),tok in zip(zip(i_idx.tolist(), j_idx.tolist()), toks)]

        # ---------- –º–µ—Ç–∫–∏ ------------------------------------------------------
        edge2order = {(int(u),int(v)): bo.nonzero(as_tuple=True)[0].item()
                    for (u,v), bo in zip(mol["edge_index"], mol["bond_orders"])}
        edge2order |= {(v,u):o for (u,v),o in edge2order.items()}

        labels = [0 if (i,j) not in edge2order else ORDER2LABEL[edge2order[(i,j)]]
                for i,j in zip(i_idx.tolist(), j_idx.tolist())]

        assistant = json.dumps([{"pair":[i,j],"label":l}
                                for (i,j),l in zip(zip(i_idx.tolist(), j_idx.tolist()),
                                                labels)],
                            separators=(",",":"))

        atom_line = "Atoms (index‚Üítype): " + \
                    ", ".join(f"{idx}:{ALLOWED[types[idx]]}"
                            for idx in range(len(types)))

        prompt = {"messages":[
            {"role":"system","content":SYSTEM},
            {"role":"user",
            "content":"Pairs within 2 √Ö:\n" + "\n".join(lines) + "\n\n" + atom_line},
            {"role":"assistant","content":assistant}
        ]}
        fout.write(json.dumps(prompt)+"\n")

